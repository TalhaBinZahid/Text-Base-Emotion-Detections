{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11rwyZKGdY7hrgaO5K3gVrlL6vuqTxL8X",
      "authorship_tag": "ABX9TyOSgmNGxKY48GB455Dd2onV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TalhaBinZahid/Text-Base-Emotion-Detections/blob/main/Copy_of_Emotions_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ****PROJECT****\n",
        "\n",
        "# ****Text Emotions Detection with Machine Learning****\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6btt-RA5eFLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "In machine learning, the detection of textual emotions is the problem of content-based classification, which is the task of natural language processing. Detecting a personâ€™s emotions is a difficult task, but detecting the emotions using text written by a person is even more difficult as a human can express his emotions in any form.\\\n",
        "Usually, emotions are expressed as joy, sadness, anger, surprise, hate, fear, etc. Recognizing this type of emotion from a text written by a person plays an important role in applications such as chatbots, customer support forum, customer reviews etc. In the section below, I will take you through a machine learning project on Text Emotions Detection using Python where I will build a machine learning model to classify the emotions of a text.\n"
      ],
      "metadata": {
        "id": "DULBIuWIfKOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def read_data(file):\n",
        "    data = []\n",
        "    with open(file, 'r')as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            label = ' '.join(line[1:line.find(\"]\")].strip().split())\n",
        "            text = line[line.find(\"]\")+1:].strip()\n",
        "            data.append([label, text])\n",
        "    return data\n",
        "\n",
        "file = '/content/drive/MyDrive/Machine Learning/emojis.txt'\n",
        "data = read_data(file)\n",
        "print(\"Number of instances: {}\".format(len(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJygzadwek9c",
        "outputId": "6d4bb807-20f2-4150-e426-6984306c82b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of instances: 7480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the data, if there any duplication or empty data."
      ],
      "metadata": {
        "id": "mEin5cRqNHU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for empty data\n",
        "if len(data) == 0:\n",
        "    print(\"Data is empty\")\n",
        "else:\n",
        "    print(\"Data is not empty\")\n",
        "\n",
        "# Check for duplicate data\n",
        "seen = set()\n",
        "duplicates = []\n",
        "for label, text in data:\n",
        "    if (label, text) in seen:\n",
        "        duplicates.append((text))\n",
        "    else:\n",
        "        seen.add((text))\n",
        "\n",
        "if duplicates:\n",
        "    print(\"Duplicate entries found:\")\n",
        "    for duplicate in duplicates:\n",
        "        print(duplicate)\n",
        "else:\n",
        "    print(\"No duplicates found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q3ex2QjNSv4",
        "outputId": "a49771af-77e2-4e7e-bd43-ce7a97236eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is not empty\n",
            "No duplicates found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I will create two Python functions for tokenization and generating the features of an input sentence:"
      ],
      "metadata": {
        "id": "wZEQeoCdgvRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram(token, n):\n",
        "    output = []\n",
        "    for i in range(n-1, len(token)):\n",
        "        ngram = ' '.join(token[i-n+1:i+1])\n",
        "        output.append(ngram)\n",
        "    return output\n",
        "\n",
        "def create_feature(text, nrange=(1, 1)):\n",
        "    text_features = []\n",
        "    text = text.lower()\n",
        "    text_alphanum = re.sub('[^a-z0-9#]', ' ', text)\n",
        "    for n in range(nrange[0], nrange[1]+1):\n",
        "        text_features += ngram(text_alphanum.split(), n)\n",
        "    # text_punc = re.sub('[a-z0-9]', ' ', text)\n",
        "    # text_features += ngram(text_punc.split(), 1)\n",
        "    return Counter(text_features)"
      ],
      "metadata": {
        "id": "Gp9BWmXAgzA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I will create a Python function to store the labels, our labels will be based on emotions such as Joy, Fear, Anger, and so on:"
      ],
      "metadata": {
        "id": "xmIJkbW2g3Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_label(item, name):\n",
        "    items = list(map(float, item.split()))\n",
        "    label = \"\"\n",
        "    for idx in range(len(items)):\n",
        "        if items[idx] == 1:\n",
        "            label += name[idx] + \" \"\n",
        "\n",
        "    return label.strip()\n",
        "\n",
        "emotions = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
        "\n",
        "X_all = []\n",
        "y_all = []\n",
        "for label, text in data:\n",
        "    y_all.append(convert_label(label, emotions))\n",
        "    X_all.append(create_feature(text, nrange=(1, 4)))"
      ],
      "metadata": {
        "id": "pXXO0ZB7g9eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I will split the data into training and test sets:"
      ],
      "metadata": {
        "id": "FNpCs9m-hBne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2, random_state = 123)\n",
        "\n",
        "def train_test(clf, X_train, X_test, y_train, y_test):\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
        "    test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
        "    return train_acc, test_acc\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "vectorizer = DictVectorizer(sparse = True)\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "qCJNA6RYhDBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Iâ€™m going to train four machine learning models and then choose the model that works best on the training and testing sets:"
      ],
      "metadata": {
        "id": "6Kcy_5-EhKza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc = SVC()\n",
        "lsvc = LinearSVC(max_iter=10000, random_state=123)\n",
        "rforest = RandomForestClassifier(random_state=123)\n",
        "dtree = DecisionTreeClassifier()\n",
        "\n",
        "clifs = [svc, lsvc, rforest, dtree]\n",
        "\n",
        "# train and test them\n",
        "print(\"| {:25} | {} | {} |\".format(\"Classifier\", \"Training Accuracy\", \"Test Accuracy\"))\n",
        "print(\"| {} | {} | {} |\".format(\"-\"*25, \"-\"*17, \"-\"*13))\n",
        "for clf in clifs:\n",
        "    clf_name = clf.__class__.__name__\n",
        "    train_acc, test_acc = train_test(clf=clf, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n",
        "    print(\"| {:25} | {:17.7f} | {:13.7f} |\".format(clf_name, train_acc, test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n92BZMQohJdu",
        "outputId": "3ec50f78-c43d-4b84-ee6e-f90c7cc842e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Classifier                | Training Accuracy | Test Accuracy |\n",
            "| ------------------------- | ----------------- | ------------- |\n",
            "| SVC                       |         0.9119318 |     0.4525401 |\n",
            "| LinearSVC                 |         0.9988302 |     0.5822193 |\n",
            "| RandomForestClassifier    |         0.9988302 |     0.5407754 |\n",
            "| DecisionTreeClassifier    |         0.9988302 |     0.4632353 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Detecting Emotion**\n",
        "Now, Iâ€™m going to assign an emoji to each label that is emotions in this problem, then Iâ€™ll write 4 input sentences, then Iâ€™ll use our trained machine learning model to take a look at the emotions of our input sentences:"
      ],
      "metadata": {
        "id": "LEaJ1_GfhOCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
        "l.sort()\n",
        "label_freq = {}\n",
        "for label, _ in data:\n",
        "    label_freq[label] = label_freq.get(label, 0) + 1\n",
        "\n",
        "# print the labels and their counts in sorted order\n",
        "for l in sorted(label_freq, key=label_freq.get, reverse=True):\n",
        "    print(\"{:10}({})  {}\".format(convert_label(l, emotions), l, label_freq[l]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76sd_PNIhYne",
        "outputId": "5cd87388-c416-48df-a7d1-4bf5d4ccecef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joy       (1. 0. 0. 0. 0. 0. 0.)  1084\n",
            "anger     (0. 0. 1. 0. 0. 0. 0.)  1080\n",
            "sadness   (0. 0. 0. 1. 0. 0. 0.)  1079\n",
            "fear      (0. 1. 0. 0. 0. 0. 0.)  1078\n",
            "disgust   (0. 0. 0. 0. 1. 0. 0.)  1057\n",
            "guilt     (0. 0. 0. 0. 0. 0. 1.)  1057\n",
            "shame     (0. 0. 0. 0. 0. 1. 0.)  1045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_dict = {\"joy\":\"ðŸ˜‚ Joy\", \"fear\":\"ðŸ˜± Fear\", \"anger\":\"ðŸ˜  Anger\", \"sadness\":\"ðŸ˜¢ Sadness\", \"disgust\":\"ðŸ˜’ Disgust\", \"shame\":\"ðŸ˜³ Shame\", \"guilt\":\"ðŸ˜³ Guilt\"}\n",
        "\n",
        "t1 = \"I feel so happy right now!\"\n",
        "t2 = \"Watching a horror movie late at night always gives me a sense of fear.\"\n",
        "t3 = \"I feel guilty for lying to you.\"\n",
        "t4 = \"The sight of the rotten food disgusted me.\"\n",
        "t5 = input(\"Enter your text for emotion detection: \")\n",
        "\n",
        "texts = [t1, t2, t3, t4, t5]\n",
        "\n",
        "for text in texts:\n",
        "    features = create_feature(text, nrange=(1, 4))\n",
        "    features = vectorizer.transform(features)\n",
        "    prediction = clf.predict(features)[0]\n",
        "    print(f\"The Text for Emotion Detection is:{text} \\nThe Predicted Emoji for text is:{emoji_dict[prediction]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL_92-JUh34Q",
        "outputId": "e3b7d440-2684-4a8e-b6ce-0a3df4e91a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your text for emotion detection: The sight of the rotten food disgusted me.\n",
            "The Text for Emotion Detection is:I feel so happy right now! \n",
            "The Predicted Emoji for text is:ðŸ˜‚ Joy\n",
            "The Text for Emotion Detection is:Watching a horror movie late at night always gives me a sense of fear. \n",
            "The Predicted Emoji for text is:ðŸ˜± Fear\n",
            "The Text for Emotion Detection is:I feel guilty for lying to you. \n",
            "The Predicted Emoji for text is:ðŸ˜³ Guilt\n",
            "The Text for Emotion Detection is:The sight of the rotten food disgusted me. \n",
            "The Predicted Emoji for text is:ðŸ˜’ Disgust\n",
            "The Text for Emotion Detection is:The sight of the rotten food disgusted me. \n",
            "The Predicted Emoji for text is:ðŸ˜’ Disgust\n"
          ]
        }
      ]
    }
  ]
}